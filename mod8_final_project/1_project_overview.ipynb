{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99af76f2",
   "metadata": {},
   "source": [
    "# Final Project: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd4c3b",
   "metadata": {},
   "source": [
    "- All in class\n",
    "- Approx 2 hours in small groups\n",
    "- Instructor guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306f98f",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression\n",
    "\n",
    "Before we jump into today's holiday-themed activity, let's briefly review **logistic regression**, a foundational model in supervised machine learning used for **classification** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### What Problem Does Logistic Regression Solve?\n",
    "\n",
    "Unlike **linear regression**, which predicts a continuous value **logistic regression predicts a probability** that an observation belongs to a particular class.\n",
    "\n",
    "> Give examples of what logistic regression could be used for?\n",
    "\n",
    "> Give examples of what linear regression could be used for?\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not Use a Line?\n",
    "\n",
    "If we tried using a normal linear regression line for classification, predictions could fall outside the range [0, 1], which doesn’t make sense for probabilities.\n",
    "\n",
    "To fix this, we apply a special mathematical function called the **sigmoid function**.\n",
    "\n",
    "---\n",
    "\n",
    "### The Sigmoid Function\n",
    "\n",
    "The sigmoid takes any real number and “squashes” it into a probability between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://cdn.britannica.com/64/264764-050-A2C174FD/graph-of-a-sigmoid-function.jpg\" width = \"800\">\n",
    "</p>\n",
    "\n",
    "Where:\n",
    "\n",
    "- $z = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n$ \n",
    "- $\\beta$’s are model parameters (weights) (i.e the thing you're estimating)\n",
    "- $x$’s are input features (i.e the data you've collected)\n",
    "\n",
    "As $z \\to +\\infty$, output $\\to$ 1  \n",
    "As $z \\to -\\infty$, output $\\to$ 0\n",
    "\n",
    "> How do you think the model would perform if the relationship between the parameters was $z = \\beta_0 + \\beta_1 cos(x_1) + \\cdots + \\beta_n x_n^2$?\n",
    "---\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "Once we have a probability, we turn it into a final prediction by applying a **threshold**:\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\sigma(z) \\ge 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For multi-class problems like today's (**naughty / nice / very nice**), we extend logistic regression using **softmax regression** (a type of multinomial logistic regression).\n",
    "\n",
    "---\n",
    "\n",
    "### Softmax for Multiclass Classification\n",
    "\n",
    "Instead of outputting just one probability, the softmax function outputs **one probability per class**, and they always sum to 1. The probability that row index $i$ is of class $k$ is: \n",
    "\n",
    "$$\n",
    "P_i(y_i = k | x_i) = \\frac{e^{z_{i,k}}}{\\sum_{j=1}^{K} e^{z_{i,j}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $z_k = \\beta_{0,k} + \\beta_{1,k}x_1 + ... + \\beta_{n,k}x_n$  \n",
    "- $K$ is the total number of classes  \n",
    "- Each class gets its own linear model output $z_k$\n",
    "\n",
    "> Say you have $n_k = 3$ classes and $n_f = 4$ features, how many model parameters does your model estimate? $(n_k*n_f)+n_k$\n",
    "---\n",
    "\n",
    "### How Does the Model Learn?\n",
    "\n",
    "Logistic regression parameters are learned by minimizing a cost function called **cross-entropy loss**, which measures how well the predicted probabilities match the actual outcomes.\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\n",
    "$$\n",
    "\n",
    "The computer updates parameters using an optimization method such as **gradient descent**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d7400",
   "metadata": {},
   "source": [
    "## Holiday Classification Problem\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://embroideres.com/files/1215/6749/7822/grinch_naughty_or_nice_machine_embroidery_design.jpg\" width = \"800\">\n",
    "</p>\n",
    "\n",
    "Santa wants a model to classify children into:\n",
    "- Naughty\n",
    "- Nice\n",
    "- Very Nice\n",
    "\n",
    "You are given the following feature dataset:\n",
    "- good_deeds — number of good deeds this year\n",
    "- tantrums — number of tantrums\n",
    "- cookies_left — how many times they left cookies for Santa\n",
    "- chores_done — % of assigned chores completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n",
      "Coefficients: [[ 1.28769755 -1.39435005  0.48294982  3.86177188]]\n",
      "Intercept: [-10.70927088]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# number of samples\n",
    "N = 300 \n",
    "\n",
    "# sample features from different distributions; really the type of distribution isn't too important here\n",
    "good_deeds = np.random.poisson(10, N)\n",
    "tantrums = np.random.poisson(5, N)\n",
    "cookies_left = np.random.binomial(10, 0.4, N)\n",
    "chores_done = np.random.uniform(0, 1, N)\n",
    "############################\n",
    "\n",
    "# define your weights and threshold for labeling the sample nice or not nice (naughty)\n",
    "# -- this is our back of the textbook solution \n",
    "weights_per_feature = {\n",
    "    \"good_deeds\": 1,\n",
    "    \"tantrums\": 1,\n",
    "    \"cookies_left\": 0.5,    \n",
    "    \"chores_done\": 5\n",
    "}\n",
    "threshold = 10\n",
    "\n",
    "# generate a label based on a simple POLYNOMIAL function of the features \n",
    "nice = (weights_per_feature['good_deeds']*good_deeds - weights_per_feature['tantrums']*tantrums + weights_per_feature['chores_done']*chores_done + weights_per_feature['cookies_left']*cookies_left > threshold).astype(int)\n",
    "\n",
    "# package things into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"good_deeds\": good_deeds,\n",
    "    \"tantrums\": tantrums,\n",
    "    \"cookies_left\": cookies_left,\n",
    "    \"chores_done\": chores_done,\n",
    "    \"nice\": nice\n",
    "})\n",
    "\n",
    "# your features\n",
    "X = df[[\"good_deeds\", \"tantrums\", \"cookies_left\", \"chores_done\"]]\n",
    "# the thing your predicting\n",
    "y = df[\"nice\"]\n",
    "\n",
    "# split training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# report the result of your model\n",
    "# you should see that your coefficients roughly match the weights you defined above \n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7a2b9",
   "metadata": {},
   "source": [
    "> In this case, what's a better metric than accuracy?\n",
    "\n",
    "> Interpret the general impact of each coefficient on the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593aaa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8933333333333333\n",
      "Coefficients: [[-1.32165936  1.29693148 -0.65767289 -2.88195674]\n",
      " [-0.03994326  0.04970594  0.02017583 -0.45794048]\n",
      " [ 1.36160263 -1.34663742  0.63749707  3.33989722]]\n"
     ]
    }
   ],
   "source": [
    "# compute your niceness score -- notice it's still polynomial in the features\n",
    "conditions = (\n",
    "    weights_per_feature['good_deeds']*good_deeds - weights_per_feature['tantrums']*tantrums + weights_per_feature['chores_done']*chores_done + weights_per_feature['cookies_left']*cookies_left\n",
    ")\n",
    "\n",
    "# define 3 classes: naughty (0), nice (1), very nice (2)\n",
    "y_multi = np.where(conditions < 5, 0, np.where(conditions < 12, 1, 2))\n",
    "df[\"label3\"] = y_multi\n",
    "\n",
    "# isolate features\n",
    "X = df[[\"good_deeds\", \"tantrums\", \"cookies_left\", \"chores_done\"]]\n",
    "# get labels\n",
    "y = df[\"label3\"]\n",
    "\n",
    "# split training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# train the model \n",
    "multi = LogisticRegression(max_iter=500)\n",
    "multi.fit(X_train, y_train)\n",
    "\n",
    "# test the model \n",
    "y_pred = multi.predict(X_test)\n",
    "\n",
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Coefficients:\", multi.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810859d7",
   "metadata": {},
   "source": [
    "> Why do we have 12 coefficient estimates here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
